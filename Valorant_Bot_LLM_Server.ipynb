{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CattoYT/ValorantCommsBot/blob/rewrite/Valorant_Bot_LLM_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT SET THIS TO EXTRA INDEX URL, YOU WILL MESS STUFF UP BADLY WIKTH PERFORAMNCE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install diskcache flask_ngrok2 pyngrok\n",
        "!pip install -U llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "from llama_cpp import Llama\n"
      ],
      "metadata": {
        "id": "AOiWHWgzkjZh",
        "outputId": "17625802-174e-4cdd-a876-4f05d66f4c52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (5.6.3)\n",
            "Requirement already satisfied: flask_ngrok2 in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok2) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok2) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok2) (2.1.5)\n",
            "Looking in indexes: https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python\n",
            "  Using cached https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.87-cu122/llama_cpp_python-0.2.87-cp310-cp310-linux_x86_64.whl (394.5 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mWUIlVBoufYe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# a little code is taken from a previous project\n",
        "class llamaLLM:\n",
        "    def __init__(self, systemprompt, model=\"bartowski/Meta-Llama-3-8B-Instruct-GGUF\", filename=\"Meta-Llama-3-8B-Instruct-Q6_K.gguf\"):\n",
        "\n",
        "\n",
        "        self.messages = [\n",
        "            # Sample data fed because I couldn't think of a way to get it to say the right shit\n",
        "          {\"role\": \"system\", \"content\": systemprompt},\n",
        "          {\"role\": \"user\", \"content\": \"(Team) teammate: clove hit 70\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"[HEALTHINDICATOR] Clove -70\"},\n",
        "          {\"role\": \"user\", \"content\": \"(Team) teammate: brim -140\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"[HEALTHINDICATOR] Brimstone -140\"},\n",
        "          {\"role\": \"user\", \"content\": \"(Team) teammate: sage 80\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"[HEALTHINDICATOR] Sage -80\"}\n",
        "        ]\n",
        "\n",
        "        self.llm = Llama.from_pretrained(\n",
        "            repo_id=model,\n",
        "            filename=filename,\n",
        "            verbose=False,\n",
        "            n_gpu_layers=-1,\n",
        "            n_ctx=4096,\n",
        "        )\n",
        "    def addToConversation(self, content, role):\n",
        "        self.messages.append(\n",
        "              {\n",
        "                  \"role\": role,\n",
        "                  \"content\": content\n",
        "              }\n",
        "          )\n",
        "    def generateConversationResponse(self, message, person=\"user\"):\n",
        "        self.addToConversation(message, person) # it can also be system\n",
        "        response = self.llm.create_chat_completion(self.messages)[\"choices\"][0]['message']['content'] # forgive me\n",
        "        self.addToConversation(response, \"assistant\")\n",
        "        return response # no genuinely what the fuck is this\n",
        "\n",
        "\n",
        "\n",
        "#while True:\n",
        "#    print(llm.generateConversationResponse(input(\">> \")))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = llamaLLM(\"\"\"\n",
        "        You are a gamer playing Valorant, and are talking in the in-game chat.\n",
        "\n",
        "        Your teammates may type out how much damage they have done to certain enemies, for example: '(Party) iopi: jett 40' or '(Party) KingAji15: sage -120'. This means that they have done 40 damage to Jett and 120 damage to Sage.\n",
        "        If this sent, you are required respond in the format of '[HEALTHINDICATOR] {Agent} {Damange}', where agent, (Any valorant agent) and damage are replaced with the agent and the amount of damage,\n",
        "        but [HEALTHINDICATOR] must be as it is in front. It should not be replaced in any way. The damage should not accumulate. Only output the new damage value and not the total damage.\n",
        "        There may be other words such as hit or damaged or any other word like that to reference damage.\n",
        "        Example responses include: '[HEALTHINDICATOR] Clove -120' or '[HEALTHINDICATOR] Astra -40'\n",
        "\n",
        "        If a teammate calls out damage without an agent, then replace the agent with ALL\n",
        "\n",
        "\n",
        "        The following are valid Valorant Agents:\n",
        "        brimstone phoenix sage sova viper cypher reyna killjoy breach omen jett raze skye yoru astra kayo chamber neon fade harbor gekko deadlock iso clove\n",
        "        There may be abbreviations used for each agent, such as KJ for killjoy or DL for deadlock.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "DTJXqVEGBoX2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import conf, ngrok\n",
        "from google.colab import userdata\n",
        "import threading\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "conf.get_default().auth_token = userdata.get('NGROKToken')\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(\"80\", domain=\"cheaply-caring-pup.ngrok-free.app\").public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, \"80\"))\n",
        "\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "# basic, will improve later\n",
        "# may need to change this later if its not needed\n",
        "class ValorantChat:\n",
        "    def __init__(self, channel, user, line):\n",
        "        self.channel = channel\n",
        "        self.user = user\n",
        "        self.line = line\n",
        "\n",
        "    def Empty(self):\n",
        "        self.channel = \"\"\n",
        "        self.user = \"\"\n",
        "        self.line = \"\"\n",
        "\n",
        "    def json(self):\n",
        "        return json.dumps(\n",
        "            {\n",
        "                \"channel\": self.channel,\n",
        "                \"user\": self.user,\n",
        "                \"line\": self.line\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def raw(self):\n",
        "        return f\"({self.channel}) {self.user}{self.line}\"\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def processChat():\n",
        "    chatmsg = json.loads(request.json)\n",
        "\n",
        "    chatmsg = ValorantChat(chatmsg[\"channel\"], chatmsg[\"user\"], chatmsg[\"line\"])\n",
        "\n",
        "    print(chatmsg.raw())\n",
        "    response = llm.generateConversationResponse(chatmsg.line)\n",
        "    print(response + \"\\n\")\n",
        "    return response\n",
        "    pass\n",
        "\n",
        "def start():\n",
        "    app.run(host='0.0.0.0', port=80)\n",
        "\n",
        "start()"
      ],
      "metadata": {
        "id": "DhYusR91acJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ce4d4f-1627-4938-e558-c800811b67e3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://cheaply-caring-pup.ngrok-free.app\" -> \"http://127.0.0.1:80\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:80\n",
            " * Running on http://172.28.0.12:80\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Party) iopi: sage -90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Aug/2024 18:40:46] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEALTHINDICATOR] Sage -90\n",
            "\n",
            "(Party) iopi: reyna -100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Aug/2024 18:40:57] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HEALTHINDICATOR] Reyna -100\n",
            "\n"
          ]
        }
      ]
    }
  ]
}