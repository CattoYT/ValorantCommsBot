{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAsdb47mGm8cGKCotG3abo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CattoYT/ValorantCommsBot/blob/rewrite/Valorant_Bot_LLM_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mWUIlVBoufYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccabf2d9-3a82-4f6d-cfff-f007eb723d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (5.6.3)\n",
            "Requirement already satisfied: flask_ngrok2 in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask_ngrok2) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok2) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask_ngrok2) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok2) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask_ngrok2) (2.1.5)\n",
            "Looking in indexes: https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.87)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            ">> (party) josh: clove hit 70\n",
            "[HEALTHINDICATOR] Clove -70\n",
            ">> (party) kaenia: hit kayo 70\n",
            "[HEALTHINDICATOR] Kayo -70\n",
            ">> (party) kaenia: kj -90\n",
            "[HEALTHINDICATOR] Killjoy -90\n",
            ">> (party) kaenia: brim -110\n",
            "[HEALTHINDICATOR] Brimstone -110\n",
            ">> (party) kaenia: Come on guys\n",
            "What's up?\n",
            ">> (party) kaenia: Lets win this game already\n",
            "Let's do this!\n",
            ">> (party) kaenia: neon -90\n",
            "[HEALTHINDICATOR] Neon -90\n",
            ">> (party) kaenia: neon -10\n",
            "[HEALTHINDICATOR] Neon -10\n",
            ">> (party) kaenia: astra hit 60\n",
            "[HEALTHINDICATOR] Astra -60\n",
            ">> (party) kaenia: astra lit 30\n",
            "[HEALTHINDICATOR] Astra -30\n",
            ">> (party) kaenia: kayo -110\n",
            "[HEALTHINDICATOR] Kayo -110\n",
            ">> (party) kaenia: kay/o 77\n",
            "[HEALTHINDICATOR] Kayo 77\n",
            ">> omen low\n",
            "Omen's health is low.\n",
            ">> (party) kaenia: I HIT KAYO 70 WTF\n",
            "[HEALTHINDICATOR] Kayo -70\n",
            ">> (party) kaenia: kayo 1 tap\n",
            "[HEALTHINDICATOR] Kayo -1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d817e90f3e7a>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateConversationResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install diskcache flask_ngrok2 pyngrok\n",
        "\n",
        "# DO NOT SET THIS TO EXTRA INDEX URL, YOU WILL MESS STUFF UP BADLY WIKTH PERFORAMNCE\n",
        "!pip install -U llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "\n",
        "\n",
        "# a little code is taken from a previous project\n",
        "from llama_cpp import Llama\n",
        "class llamaLLM:\n",
        "    def __init__(self, systemprompt, model=\"bartowski/Meta-Llama-3-8B-Instruct-GGUF\", filename=\"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"):\n",
        "\n",
        "\n",
        "        self.messages = [\n",
        "            # Sample data fed because I couldn't think of a way to get it to say the right shit\n",
        "          {\"role\": \"system\", \"content\": systemprompt},\n",
        "          {\"role\": \"user\", \"content\": \"(party) josh: clove hit 70\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"[HEALTHINDICATOR] Clove -70\"},\n",
        "          {\"role\": \"user\", \"content\": \"(party) josh: brim -140\"},\n",
        "          {\"role\": \"assistant\", \"content\": \"[HEALTHINDICATOR] Brimstone -140\"},\n",
        "        ]\n",
        "\n",
        "        self.llm = Llama.from_pretrained(\n",
        "            repo_id=model,\n",
        "            filename=filename,\n",
        "            verbose=False,\n",
        "            n_gpu_layers=-1,\n",
        "            n_ctx=4096\n",
        "        )\n",
        "    def addToConversation(self, content, role):\n",
        "        self.messages.append(\n",
        "              {\n",
        "                  \"role\": role,\n",
        "                  \"content\": content\n",
        "              }\n",
        "          )\n",
        "    def generateConversationResponse(self, message, person=\"user\"):\n",
        "        self.addToConversation(message, person) # it can also be system\n",
        "        response = self.llm.create_chat_completion(self.messages)[\"choices\"][0]['message']['content'] # forgive me\n",
        "        self.addToConversation(response, \"assistant\")\n",
        "        return response # no genuinely what the fuck is this\n",
        "\n",
        "llm = llamaLLM(\"\"\"\n",
        "You are a gamer playing Valorant, and are talking in the in-game chat. Your teammates may callout damage to certain agents like 'jett 40' or 'sage -120'.\n",
        "If this sent, you must respond in the format of \"[HEALTHINDICATOR] {Agent} {Damange}\", where agent, (Any valorant agent) and damage are replaced with the agent and the amount of damage,\n",
        "but [HEALTHINDICATOR] must be as it is in front. It should not be replaced in any way. The damage should not accumulate. Only output the new damage value and not the total damage\n",
        "\n",
        "There may be other words such as hit or damaged or any other word like that to reference damage.\n",
        "Most other information should be removed if it doesn't fit this format\n",
        "When the message you need to respond to is not a form of callout, in location or damage, you can respond naturally.\n",
        "\n",
        "The following are valid Valorant Agents:\n",
        "brimstone phoenix sage sova viper cypher reyna killjoy breach omen jett raze skye yoru astra kayo chamber neon fade harbor gekko deadlock iso clove\n",
        "There may be abbreviations used for each agent, such as KJ for killjoy or DL for deadlock.\n",
        "\"\"\")\n",
        "\n",
        "while True:\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "import speaker\n",
        "app = Flask(__name__)\n",
        "# basic, will improve later\n",
        "\n",
        "@app.route('/')\n",
        "def processChat():\n",
        "    chatmsg = request.args.get(\"chatmsg\")\n",
        "\n",
        "    return llm.generateConversationResponse(chatmsg)\n",
        "    pass\n",
        "\n",
        "def start():\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "DhYusR91acJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}